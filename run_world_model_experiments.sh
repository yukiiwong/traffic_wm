#!/usr/bin/env bash
set -euo pipefail

# é¿å…æ˜¾å­˜ç¢ç‰‡å¯¼è‡´çš„ OOM
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# ====== è·¯å¾„è®¾ç½® ======
PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
cd "$PROJECT_ROOT"

TRAIN_SCRIPT="src/training/train_world_model.py"

TRAIN_DATA="data/processed/train_episodes.npz"
VAL_DATA="data/processed/val_episodes.npz"

LOG_DIR="logs/world_model"
CKPT_DIR="checkpoints/world_model"

mkdir -p "$LOG_DIR" "$CKPT_DIR"

# ====== GPU è®¾ç½®ï¼ˆè¿™é‡Œç”¨ 0 å’Œ 1 ä¸¤å—å¡ï¼‰======
GPU_IDS=(0 1)                    # å¯ç”¨ GPU åˆ—è¡¨
NUM_GPUS=${#GPU_IDS[@]}          # GPU æ•°é‡
MAX_JOBS=$NUM_GPUS               # åŒæ—¶æœ€å¤šè·‘å‡ ä¸ªå®éªŒï¼ˆè¿™é‡Œ = GPU æ•°ï¼‰

JOB_IDX=0                        # å®éªŒè®¡æ•°å™¨ï¼Œç”¨äºè½®æµåˆ†é… GPU

# ====== ä» metadata.json è‡ªåŠ¨è¯»å– input_dim ======
INPUT_DIM=$(python - << 'PY'
import json
with open("data/processed/metadata.json") as f:
    meta = json.load(f)
print(meta["n_features"])
PY
)

echo "Detected INPUT_DIM from metadata.json: ${INPUT_DIM}"

# ====== å›ºå®šè®­ç»ƒè¶…å‚ï¼ˆæŒ‰éœ€ä¿®æ”¹ï¼‰ ======
BATCH_SIZE=128          # ç¨å¾®ä¿å®ˆï¼Œé˜²æ­¢ OOMï¼›ä½ å¯ä»¥è¯•ç€æ”¹å› 128
N_EPOCHS=200
LR=3e-4

LATENT_DIMS=(256 512)                 # å¯¹æ¯”ä¸¤ç§ latent_dim
DYNAMICS_TYPES=("gru" "lstm" "transformer")  # ä¸‰ç§ dynamics
SEEDS=(1 2 3)                         # å¤šéšæœºç§å­

# ====== å¹¶è¡Œæ§åˆ¶ï¼šé™åˆ¶åå°ä»»åŠ¡æ•° ======
wait_for_free_slot() {
  while true; do
    local njobs
    njobs=$(jobs -r | wc -l)   # å½“å‰æ­£åœ¨è¿è¡Œçš„åå°ä»»åŠ¡æ•°
    if (( njobs < MAX_JOBS )); then
      break
    fi
    sleep 5
  done
}

# ====== ä¸»å¾ªç¯ï¼šæ‰«æ‰€æœ‰ç»„åˆï¼Œå¹¶è½®æµåˆ†é…åˆ°ä¸åŒ GPU ======
for dyn in "${DYNAMICS_TYPES[@]}"; do
  for latent in "${LATENT_DIMS[@]}"; do
    for seed in "${SEEDS[@]}"; do

      EXP_NAME="dyn_${dyn}_z${latent}_seed${seed}"
      LOG_FILE="${LOG_DIR}/${EXP_NAME}.log"
      CKPT_SUBDIR="${CKPT_DIR}/${EXP_NAME}"
      mkdir -p "$CKPT_SUBDIR"

      echo "â–¶ Starting experiment: ${EXP_NAME}"
      echo "  dynamics_type=${dyn}, latent_dim=${latent}, seed=${seed}"
      echo "  logs:   ${LOG_FILE}"
      echo "  ckpt:   ${CKPT_SUBDIR}"

      # ç­‰å¾…æœ‰ç©ºé—²â€œæ§½ä½â€ï¼ˆä¸è¶…è¿‡ MAX_JOBS ä¸ªå¹¶è¡Œä»»åŠ¡ï¼‰
      wait_for_free_slot

      # è½®æµé€‰æ‹© GPUï¼š0,1,0,1,...
      GPU_ID=${GPU_IDS[$(( JOB_IDX % NUM_GPUS ))]}
      JOB_IDX=$(( JOB_IDX + 1 ))
      echo "  using GPU: ${GPU_ID}"

      # å¯åŠ¨è®­ç»ƒï¼ˆæ”¾åˆ°åå°è·‘ï¼Œè¾“å‡ºå†™å…¥ logï¼‰
      CUDA_VISIBLE_DEVICES=${GPU_ID} \
      python "$TRAIN_SCRIPT" \
        --train_data     "$TRAIN_DATA" \
        --val_data       "$VAL_DATA" \
        --input_dim      "$INPUT_DIM" \
        --latent_dim     "$latent" \
        --dynamics_type  "$dyn" \
        --batch_size     "$BATCH_SIZE" \
        --n_epochs       "$N_EPOCHS" \
        --learning_rate  "$LR" \
        --seed           "$seed" \
        --checkpoint_dir "$CKPT_SUBDIR" \
        --log_dir        "$CKPT_SUBDIR" \
        >"$LOG_FILE" 2>&1 &

    done
  done
done

echo "ğŸ‰ All experiments submitted. Use 'nvidia-smi' to monitor GPUs, and 'tail -f logs/world_model/xxx.log' to watch training."

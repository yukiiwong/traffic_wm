# World Model æ¶æ„å¯¹æ¯”: Traffic WM vs DreamerV3

**æ—¥æœŸ**: 2025-12-14
**ç‰ˆæœ¬**: 1.0

---

## ğŸ“‹ ç›®å½•

1. [æ€»ä½“æ¶æ„å¯¹æ¯”](#æ€»ä½“æ¶æ„å¯¹æ¯”)
2. [Encoder è®¾è®¡å¯¹æ¯”](#encoder-è®¾è®¡å¯¹æ¯”)
3. [Dynamics æ¨¡å‹å¯¹æ¯”](#dynamics-æ¨¡å‹å¯¹æ¯”)
4. [Decoder è®¾è®¡å¯¹æ¯”](#decoder-è®¾è®¡å¯¹æ¯”)
5. [è®­ç»ƒç­–ç•¥å¯¹æ¯”](#è®­ç»ƒç­–ç•¥å¯¹æ¯”)
6. [å…³é”®å·®å¼‚æ€»ç»“](#å…³é”®å·®å¼‚æ€»ç»“)
7. [ä¼˜ç¼ºç‚¹åˆ†æ](#ä¼˜ç¼ºç‚¹åˆ†æ)

---

## 1. æ€»ä½“æ¶æ„å¯¹æ¯”

### 1.1 Traffic WM (å½“å‰å®ç°)

```
è¾“å…¥: Multi-agent states [B, T, K, F]
  â†“
Encoder: MultiAgentEncoder
  - Feature Embedding (è¿ç»­ç‰¹å¾)
  - Site Embedding (ç¦»æ•£site_id)
  - Lane Embedding (ç¦»æ•£lane_id)
  - Transformer Attention (è·¨agentç»´åº¦K)
  - Masked Mean Pooling â†’ [B, T, latent_dim]
  â†“
Dynamics: LatentDynamics (GRU/LSTM/Transformer)
  - 1-step transition: z[t] â†’ z[t+1]
  - Teacher forcing during training
  - Open-loop rollout during evaluation
  â†“
Decoder: StateDecoder
  - MLP layers: latent â†’ states + existence logits
  - Outputs: [B, T, K, F] states, [B, T, K] masks
  â†“
Loss:
  - Reconstruction Loss: L2(states_t, reconstructed_t)
  - Prediction Loss: L2(states_{t+1}, predicted_{t+1})
  - Existence Loss: BCE(masks, predicted_masks)
```

**å…³é”®ç‰¹ç‚¹**:
- **ç¡®å®šæ€§æ¨¡å‹**: å®Œå…¨ç¡®å®šæ€§çš„latentè¡¨ç¤º
- **å¤šæ™ºèƒ½ä½“ç‰¹åŒ–**: ä¸“é—¨è®¾è®¡ç”¨äºå¤šagentè½¨è¿¹é¢„æµ‹
- **ç®€å•æ¶æ„**: ç›´æ¥çš„encoder-dynamics-decoderç»“æ„
- **PyTorchå®ç°**: ä½¿ç”¨PyTorchæ¡†æ¶

### 1.2 DreamerV3

```
è¾“å…¥: Observations (images + vectors)
  â†“
Encoder: MultiEncoder
  - CNN Encoder (ResNet) for images
  - MLP Encoder for vectors
  - Concatenate features â†’ embed
  â†“
RSSM (Recurrent State-Space Model):
  - Deterministic: GRU(stoch_{t-1}, action_{t-1}) â†’ deter_t
  - Stochastic (Prior): MLP(deter_t) â†’ prior_logits_t
  - Stochastic (Posterior): MLP(deter_t, embed_t) â†’ post_logits_t
  - Sample: stoch_t ~ Categorical(post_logits_t)
  - Latent = [deter_t, stoch_t]
  â†“
Decoder: MultiDecoder
  - CNN Decoder (ResNet) for images
  - MLP Decoder for vectors
  - Reward Head
  - Continuation Head
  â†“
Loss:
  - Dynamics Loss: KL(posterior || prior)
  - Representation Loss: KL(posterior || sg(prior))
  - Reconstruction Loss: -log p(obs | latent)
  - Reward Loss: -log p(reward | latent)
  - Continuation Loss: -log p(cont | latent)
```

**å…³é”®ç‰¹ç‚¹**:
- **éšæœºæ¨¡å‹**: RSSMç»“åˆç¡®å®šæ€§(deter)å’Œéšæœºæ€§(stoch)
- **å¼ºåŒ–å­¦ä¹ ç‰¹åŒ–**: è®¾è®¡ç”¨äºmodel-based RL
- **å¤æ‚æ¶æ„**: åˆ†ç¦»çš„priorå’Œposterior,å¤šä¸ªhead
- **JAXå®ç°**: ä½¿ç”¨JAXæ¡†æ¶,æ”¯æŒJITç¼–è¯‘

---

## 2. Encoder è®¾è®¡å¯¹æ¯”

### 2.1 Traffic WM: MultiAgentEncoder

**æ–‡ä»¶**: `src/models/encoder.py`

```python
class MultiAgentEncoder(nn.Module):
    def __init__(self, input_dim, hidden_dim, latent_dim, n_layers, dropout, use_site_id):
        # 1. Feature Embedding
        self.feature_embedding = nn.Linear(input_dim - num_discrete, hidden_dim)

        # 2. Discrete Embeddings
        self.site_embedding = nn.Embedding(num_sites, site_embed_dim)
        self.lane_embedding = nn.Embedding(num_lanes, lane_embed_dim)

        # 3. Transformer Layers (è·¨agentæ³¨æ„åŠ›)
        self.transformer_layers = nn.ModuleList([
            nn.TransformerEncoderLayer(
                d_model=hidden_dim,
                nhead=8,
                dim_feedforward=hidden_dim * 4,
                dropout=dropout,
                batch_first=True
            )
            for _ in range(n_layers)
        ])

        # 4. Output Projection
        self.to_latent = nn.Sequential(
            nn.Linear(hidden_dim, latent_dim),
            nn.LayerNorm(latent_dim)
        )

    def forward(self, states, masks):
        B, T, K, F = states.shape

        # æå–ç¦»æ•£ç‰¹å¾
        site_id = states[..., site_idx].long()
        lane_id = states[..., lane_idx].long()

        # Embedding
        cont_feats = self.feature_embedding(states[..., continuous_indices])
        site_embed = self.site_embedding(site_id)
        lane_embed = self.lane_embedding(lane_id)

        # æ‹¼æ¥
        x = cont_feats + site_embed + lane_embed  # [B, T, K, hidden_dim]

        # Transformer (è·¨Kç»´åº¦)
        for layer in self.transformer_layers:
            x = layer(x, src_key_padding_mask=~masks.bool())

        # Pooling (è·¨Kç»´åº¦)
        x = masked_mean_pooling(x, masks)  # [B, T, hidden_dim]

        # Project to latent
        latent = self.to_latent(x)  # [B, T, latent_dim]

        return latent
```

**ç‰¹ç‚¹**:
- âœ… **å¤šæ™ºèƒ½ä½“èšåˆ**: Transformer attentionè·¨agentç»´åº¦
- âœ… **ç¦»æ•£ç‰¹å¾å¤„ç†**: ä¸“é—¨çš„embeddingå±‚ç”¨äºsite_idå’Œlane_id
- âœ… **Masked pooling**: æ­£ç¡®å¤„ç†paddingçš„agent
- âœ… **ç¡®å®šæ€§è¾“å‡º**: ç›´æ¥è¾“å‡ºlatentå‘é‡,æ— éšæœºæ€§
- âš ï¸ **æ— CNN**: ä¸å¤„ç†å›¾åƒè¾“å…¥,ä»…å¤„ç†å‘é‡ç‰¹å¾

### 2.2 DreamerV3: MultiEncoder

**æ–‡ä»¶**: `dreamerv3/nets.py:211-260`

```python
class MultiEncoder(nj.Module):
    def __init__(self, shapes, cnn_keys, mlp_keys, mlp_layers, mlp_units,
                 cnn='resnet', cnn_depth=48, cnn_blocks=2, ...):
        # åˆ†ç¦»CNNå’ŒMLPè¾“å…¥
        self.cnn_shapes = {k: v for k, v in shapes.items() if len(v) == 3}  # Images
        self.mlp_shapes = {k: v for k, v in shapes.items() if len(v) in (1, 2)}  # Vectors

        # CNN: ResNetç¼–ç å™¨
        if cnn == 'resnet':
            self._cnn = ImageEncoderResnet(cnn_depth, cnn_blocks, ...)

        # MLP: å¤šå±‚æ„ŸçŸ¥æœº
        if self.mlp_shapes:
            self._mlp = MLP(None, mlp_layers, mlp_units, dist='none')

    def __call__(self, data):
        outputs = []

        # CNNç¼–ç å›¾åƒ
        if self.cnn_shapes:
            inputs = jnp.concatenate([data[k] for k in self.cnn_shapes], -1)
            output = self._cnn(inputs)
            outputs.append(output.reshape((output.shape[0], -1)))

        # MLPç¼–ç å‘é‡
        if self.mlp_shapes:
            inputs = jnp.concatenate([data[k] for k in self.mlp_shapes], -1)
            outputs.append(self._mlp(inputs))

        # æ‹¼æ¥æ‰€æœ‰ç¼–ç 
        return jnp.concatenate(outputs, -1)
```

**ç‰¹ç‚¹**:
- âœ… **å¤šæ¨¡æ€è¾“å…¥**: åŒæ—¶å¤„ç†å›¾åƒ(CNN)å’Œå‘é‡(MLP)
- âœ… **ResNetæ¶æ„**: ä½¿ç”¨æ·±åº¦æ®‹å·®ç½‘ç»œç¼–ç å›¾åƒ
- âœ… **çµæ´»è®¾è®¡**: é€šè¿‡regexåŒ¹é…åŠ¨æ€é€‰æ‹©CNN/MLPè¾“å…¥
- âš ï¸ **æ— å¤šæ™ºèƒ½ä½“èšåˆ**: å‡è®¾å•agentæˆ–å·²èšåˆçš„è§‚æµ‹
- âš ï¸ **æ— ç¦»æ•£ç‰¹å¾å¤„ç†**: éœ€è¦one-hotæˆ–é¢„å¤„ç†

---

## 3. Dynamics æ¨¡å‹å¯¹æ¯”

### 3.1 Traffic WM: LatentDynamics

**æ–‡ä»¶**: `src/models/dynamics.py`

```python
class LatentDynamics(nn.Module):
    def __init__(self, latent_dim, hidden_dim, n_layers, dropout, model_type):
        if model_type == 'gru':
            self.rnn = nn.GRU(
                input_size=latent_dim,
                hidden_size=hidden_dim,
                num_layers=n_layers,
                dropout=dropout if n_layers > 1 else 0,
                batch_first=True
            )
        elif model_type == 'lstm':
            self.rnn = nn.LSTM(...)
        elif model_type == 'transformer':
            self.transformer = nn.TransformerEncoder(...)

        # Output projection
        self.output_proj = nn.Linear(hidden_dim, latent_dim)

    def forward(self, latent, hidden=None):
        # latent: [B, T, latent_dim]

        if self.model_type == 'transformer':
            output = self.transformer(latent)
            return output, None
        else:  # GRU or LSTM
            rnn_out, hidden = self.rnn(latent, hidden)
            output = self.output_proj(rnn_out)  # [B, T, latent_dim]
            return output, hidden
```

**ç‰¹ç‚¹**:
- âœ… **ç®€å•ç›´æ¥**: å•ä¸€çš„RNNæˆ–Transformeræ¨¡å‹
- âœ… **ç¡®å®šæ€§è½¬æ¢**: latent[t] â†’ latent[t+1]
- âœ… **çµæ´»é€‰æ‹©**: æ”¯æŒGRU/LSTM/Transformerä¸‰ç§dynamics
- âœ… **æ— actionè¾“å…¥**: ä¸éœ€è¦action,é€‚ç”¨äºçº¯é¢„æµ‹ä»»åŠ¡
- âš ï¸ **æ— éšæœºæ€§**: å®Œå…¨ç¡®å®šæ€§,å¯èƒ½æ¬ æ‹Ÿåˆå¤æ‚åŠ¨æ€

### 3.2 DreamerV3: RSSM (Recurrent State-Space Model)

**æ–‡ä»¶**: `dreamerv3/nets.py:22-209`

```python
class RSSM(nj.Module):
    def __init__(self, deter=1024, stoch=32, classes=32, ...):
        self._deter = deter  # ç¡®å®šæ€§çŠ¶æ€ç»´åº¦
        self._stoch = stoch  # éšæœºçŠ¶æ€ç»´åº¦
        self._classes = classes  # ç¦»æ•£ç±»åˆ«æ•°

    def obs_step(self, prev_state, prev_action, embed, is_first):
        """è§‚æµ‹æ­¥éª¤: ç»™å®šè§‚æµ‹embed,æ›´æ–°çŠ¶æ€"""

        # 1. Prior: img_stepé¢„æµ‹ä¸‹ä¸€çŠ¶æ€
        prior = self.img_step(prev_state, prev_action)

        # 2. Posterior: ç»“åˆè§‚æµ‹ä¿®æ­£
        x = jnp.concatenate([prior['deter'], embed], -1)
        x = self.get('obs_out', Linear)(x)
        stats = self._stats('obs_stats', x)  # â†’ logits or (mean, std)

        # 3. Sample stochastic state
        dist = self.get_dist(stats)
        stoch = dist.sample(seed=nj.rng())

        post = {'stoch': stoch, 'deter': prior['deter'], **stats}
        return post, prior

    def img_step(self, prev_state, prev_action):
        """æƒ³è±¡æ­¥éª¤: ä»…åŸºäºactioné¢„æµ‹ä¸‹ä¸€çŠ¶æ€"""

        prev_stoch = prev_state['stoch']

        # 1. GRUæ›´æ–°ç¡®å®šæ€§çŠ¶æ€
        x = jnp.concatenate([prev_stoch, prev_action], -1)
        x = self.get('img_in', Linear)(x)
        x, deter = self._gru(x, prev_state['deter'])

        # 2. é¢„æµ‹éšæœºçŠ¶æ€çš„prior
        x = self.get('img_out', Linear)(x)
        stats = self._stats('img_stats', x)
        dist = self.get_dist(stats)
        stoch = dist.sample(seed=nj.rng())

        prior = {'stoch': stoch, 'deter': deter, **stats}
        return prior

    def _gru(self, x, deter):
        """è‡ªå®šä¹‰GRUå®ç°"""
        x = jnp.concatenate([deter, x], -1)
        x = self.get('gru', Linear, units=3 * self._deter)(x)
        reset, cand, update = jnp.split(x, 3, -1)

        reset = jax.nn.sigmoid(reset)
        cand = jnp.tanh(reset * cand)
        update = jax.nn.sigmoid(update - 1)

        deter = update * cand + (1 - update) * deter
        return deter, deter
```

**RSSMæ ¸å¿ƒæ€æƒ³**:

```
Latent State = [Deterministic, Stochastic]
             = [deter_t,      stoch_t     ]

æ—¶é—´æ¼”åŒ–:
  deter_t = GRU(deter_{t-1}, [stoch_{t-1}, action_{t-1}])

  prior_t ~ p(stoch_t | deter_t)           # ä»…åŸºäºå†å²
  post_t  ~ p(stoch_t | deter_t, embed_t)  # ç»“åˆå½“å‰è§‚æµ‹
```

**ç‰¹ç‚¹**:
- âœ… **éšæœº+ç¡®å®š**: åˆ†ç¦»ç¡®å®šæ€§è®°å¿†å’Œéšæœºæ€§å˜åŒ–
- âœ… **Prior-Posterior**: åˆ†åˆ«å»ºæ¨¡é¢„æµ‹å’Œä¿®æ­£
- âœ… **ç¦»æ•£éšæœº**: ä½¿ç”¨Categoricalåˆ†å¸ƒ(æ›´ç¨³å®š)æˆ–Gaussian
- âœ… **Action-conditioned**: æ˜¾å¼å»ºæ¨¡actionçš„å½±å“
- âš ï¸ **å¤æ‚**: éœ€è¦ç»´æŠ¤ä¸¤ä¸ªåˆ†å¸ƒ,è®­ç»ƒæ›´å¤æ‚
- âš ï¸ **éœ€è¦action**: ä¸é€‚ç”¨äºçº¯è§‚æµ‹é¢„æµ‹ä»»åŠ¡

**å…³é”®å·®å¼‚**:

| æ–¹é¢ | Traffic WM | DreamerV3 RSSM |
|------|-----------|---------------|
| **çŠ¶æ€è¡¨ç¤º** | çº¯ç¡®å®šæ€§ latent | ç¡®å®šæ€§deter + éšæœºstoch |
| **åŠ¨æ€æ¨¡å‹** | RNNç›´æ¥é¢„æµ‹latent | GRUæ›´æ–°deter,ç„¶åé‡‡æ ·stoch |
| **éšæœºæ€§** | æ—  | æœ‰(Categoricalæˆ–Gaussian) |
| **Action** | ä¸éœ€è¦ | å¿…é¡»(action-conditioned) |
| **Prior/Posterior** | æ—  | æœ‰(åˆ†åˆ«å»ºæ¨¡) |
| **KL Loss** | æ—  | æœ‰(çº¦æŸpriorå’Œposterior) |

---

## 4. Decoder è®¾è®¡å¯¹æ¯”

### 4.1 Traffic WM: StateDecoder

**æ–‡ä»¶**: `src/models/decoder.py`

```python
class StateDecoder(nn.Module):
    def __init__(self, latent_dim, hidden_dim, output_dim, max_agents, n_layers, dropout):
        # MLP decoder
        layers = []
        current_dim = latent_dim

        for _ in range(n_layers):
            layers.extend([
                nn.Linear(current_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout)
            ])
            current_dim = hidden_dim

        self.decoder = nn.Sequential(*layers)

        # Output heads
        self.state_head = nn.Linear(hidden_dim, max_agents * output_dim)
        self.existence_head = nn.Linear(hidden_dim, max_agents)

    def forward(self, latent):
        # latent: [B, T, latent_dim]

        # Decode
        x = self.decoder(latent)  # [B, T, hidden_dim]

        # State prediction
        states = self.state_head(x)  # [B, T, max_agents * output_dim]
        states = states.reshape(B, T, max_agents, output_dim)

        # Existence prediction
        existence_logits = self.existence_head(x)  # [B, T, max_agents]

        return states, existence_logits
```

**ç‰¹ç‚¹**:
- âœ… **ç®€å•MLP**: å¤šå±‚å…¨è¿æ¥ç½‘ç»œ
- âœ… **å¤šæ™ºèƒ½ä½“è¾“å‡º**: ç›´æ¥è¾“å‡º[K, F]å½¢çŠ¶çš„states
- âœ… **Existence prediction**: é¢„æµ‹æ¯ä¸ªagent slotæ˜¯å¦å­˜åœ¨
- âœ… **ç¡®å®šæ€§è¾“å‡º**: ç›´æ¥è¾“å‡ºmean,æ— åˆ†å¸ƒ
- âš ï¸ **æ— å›¾åƒé‡å»º**: ä»…è¾“å‡ºå‘é‡çŠ¶æ€

### 4.2 DreamerV3: MultiDecoder

**æ–‡ä»¶**: `dreamerv3/nets.py:263-329`

```python
class MultiDecoder(nj.Module):
    def __init__(self, shapes, cnn='resnet', cnn_depth=48, mlp_layers=4,
                 image_dist='mse', vector_dist='mse', ...):
        # åˆ†ç¦»CNNå’ŒMLPè¾“å‡º
        self.cnn_shapes = {k: v for k, v in shapes.items() if len(v) == 3}
        self.mlp_shapes = {k: v for k, v in shapes.items() if len(v) == 1}

        # CNN Decoder
        if self.cnn_shapes:
            if cnn == 'resnet':
                self._cnn = ImageDecoderResnet(shape, cnn_depth, ...)

        # MLP Decoder
        if self.mlp_shapes:
            self._mlp = MLP(self.mlp_shapes, mlp_layers, mlp_units, ...)

        self._image_dist = image_dist

    def __call__(self, inputs):
        features = self._inputs(inputs)  # ä»latentæå–ç‰¹å¾
        dists = {}

        # CNNè§£ç å›¾åƒ
        if self.cnn_shapes:
            output = self._cnn(features)
            means = jnp.split(output, split_indices, -1)
            dists.update({
                key: self._make_image_dist(key, mean)
                for (key, shape), mean in zip(self.cnn_shapes.items(), means)
            })

        # MLPè§£ç å‘é‡
        if self.mlp_shapes:
            dists.update(self._mlp(features))

        return dists  # è¿”å›åˆ†å¸ƒå­—å…¸

    def _make_image_dist(self, name, mean):
        if self._image_dist == 'normal':
            return tfd.Independent(tfd.Normal(mean, 1), 3)
        if self._image_dist == 'mse':
            return jaxutils.MSEDist(mean, 3, 'sum')
```

**ç‰¹ç‚¹**:
- âœ… **å¤šæ¨¡æ€è¾“å‡º**: CNNé‡å»ºå›¾åƒ,MLPé‡å»ºå‘é‡
- âœ… **ResNet Decoder**: ä½¿ç”¨æ·±åº¦æ®‹å·®ç½‘ç»œè§£ç å›¾åƒ
- âœ… **æ¦‚ç‡è¾“å‡º**: è¾“å‡ºåˆ†å¸ƒ(Normalæˆ–MSE),ä¸æ˜¯å•ç‚¹ä¼°è®¡
- âœ… **çµæ´»**: æ”¯æŒå¤šç§è¾“å‡ºç±»å‹å’Œåˆ†å¸ƒ
- âš ï¸ **æ— å¤šæ™ºèƒ½ä½“**: å‡è®¾å•agentæˆ–å·²èšåˆè¾“å‡º

---

## 5. è®­ç»ƒç­–ç•¥å¯¹æ¯”

### 5.1 Traffic WM: ç›‘ç£å­¦ä¹ 

**æ–‡ä»¶**: `src/training/train_world_model.py`, `src/training/losses.py`

```python
class WorldModelLoss(nn.Module):
    def forward(self, predictions, targets):
        states = targets['states']
        masks = targets['masks']

        # 1. Reconstruction Loss
        recon_loss = masked_huber_loss(
            predictions['reconstructed_states'],
            states,
            masks
        )

        # 2. One-step Prediction Loss (æ—¶é—´åç§»)
        pred_states = predictions['predicted_states'][:, :-1]
        target_states = states[:, 1:]
        target_masks = masks[:, 1:]

        pred_loss = masked_huber_loss(
            pred_states, target_states, target_masks
        )

        # 3. Existence Loss
        existence_loss = BCE(
            predictions['existence_logits'],
            masks
        )

        # Total Loss
        total_loss = (
            self.recon_weight * recon_loss +
            self.pred_weight * pred_loss +
            self.existence_weight * existence_loss
        )

        return {'total': total_loss, ...}
```

**è®­ç»ƒæµç¨‹**:
```python
# Forward pass
states, masks = batch['states'], batch['masks']
output = model(states, masks)

# Compute loss
loss = loss_fn(output, {'states': states, 'masks': masks})

# Backprop
loss['total'].backward()
optimizer.step()
```

**ç‰¹ç‚¹**:
- âœ… **ç®€å•ç›´æ¥**: æ ‡å‡†ç›‘ç£å­¦ä¹ 
- âœ… **Teacher forcing**: è®­ç»ƒæ—¶ä½¿ç”¨çœŸå®latentåºåˆ—
- âœ… **æ˜ç¡®ç›®æ ‡**: é‡å»ºå½“å‰å¸§ + é¢„æµ‹ä¸‹ä¸€å¸§
- âš ï¸ **Exposure bias**: æµ‹è¯•æ—¶çœ‹ä¸åˆ°çœŸå®latent
- âš ï¸ **æ— æ­£åˆ™åŒ–**: ç¼ºå°‘latentç©ºé—´çš„çº¦æŸ

### 5.2 DreamerV3: å˜åˆ†æ¨æ–­ + RL

**æ–‡ä»¶**: `dreamerv3/agent.py:154-187`

```python
class WorldModel:
    def loss(self, data, state):
        # 1. Encode observations
        embed = self.encoder(data)

        # 2. RSSM observe (å¾—åˆ°posteriorå’Œprior)
        post, prior = self.rssm.observe(
            embed, prev_actions, data['is_first'], prev_latent
        )

        # 3. Decode from posterior
        dists = {}
        feats = {**post, 'embed': embed}
        for name, head in self.heads.items():
            out = head(feats)
            dists.update(out)

        # 4. Compute losses
        losses = {}

        # Dynamics Loss: KL(posterior || prior)
        losses['dyn'] = self.rssm.dyn_loss(post, prior, impl='kl', free=1.0)

        # Representation Loss: KL(posterior || sg(prior))
        losses['rep'] = self.rssm.rep_loss(post, prior, impl='kl', free=1.0)

        # Reconstruction Losses
        for key, dist in dists.items():
            loss = -dist.log_prob(data[key])
            losses[key] = loss

        # 5. Weighted sum
        scaled = {k: v * self.scales[k] for k, v in losses.items()}
        model_loss = sum(scaled.values())

        return model_loss.mean()
```

**å…³é”®Lossç»„ä»¶**:

1. **Dynamics Loss (KL Divergence)**:
   ```
   L_dyn = KL(posterior_t || prior_t)
         = KL(q(stoch_t | deter_t, obs_t) || p(stoch_t | deter_t))
   ```
   - çº¦æŸposteriorä¸è¦åç¦»priorå¤ªè¿œ
   - é˜²æ­¢posteriorè¿‡åº¦ä¾èµ–è§‚æµ‹

2. **Representation Loss**:
   ```
   L_rep = KL(posterior_t || sg(prior_t))
   ```
   - sg() = stop gradient
   - è®­ç»ƒposterioræ‹Ÿåˆæ•°æ®,ä¸å½±å“prior

3. **Reconstruction Loss**:
   ```
   L_recon = -log p(obs_t | latent_t)
           = -log p(obs_t | deter_t, stoch_t)
   ```

**ç‰¹ç‚¹**:
- âœ… **å˜åˆ†æ¨æ–­**: å­¦ä¹ latentçš„æ¦‚ç‡åˆ†å¸ƒ
- âœ… **KLæ­£åˆ™åŒ–**: çº¦æŸlatentç©ºé—´ç»“æ„
- âœ… **Posterior-Prior**: åˆ†ç¦»è§‚æµ‹ç¼–ç å’ŒåŠ¨æ€é¢„æµ‹
- âœ… **Free bits**: KL lossä¸‹ç•Œ,é˜²æ­¢posterior collapse
- âš ï¸ **å¤æ‚**: éœ€è¦å¹³è¡¡å¤šä¸ªlossæƒé‡
- âš ï¸ **RLå¯¼å‘**: è¿˜æœ‰actor-criticè®­ç»ƒ(æœªåœ¨æ­¤å±•ç¤º)

---

## 6. å…³é”®å·®å¼‚æ€»ç»“

### 6.1 è®¾è®¡å“²å­¦

| æ–¹é¢ | Traffic WM | DreamerV3 |
|------|-----------|-----------|
| **ç›®æ ‡ä»»åŠ¡** | å¤šæ™ºèƒ½ä½“è½¨è¿¹é¢„æµ‹ | å¼ºåŒ–å­¦ä¹ (model-based RL) |
| **æ•°æ®ç±»å‹** | å‘é‡ç‰¹å¾(åæ ‡ã€é€Ÿåº¦ç­‰) | å›¾åƒ + å‘é‡ |
| **éšæœºæ€§** | ç¡®å®šæ€§æ¨¡å‹ | éšæœºæ¨¡å‹(RSSM) |
| **Action** | ä¸éœ€è¦ | å¿…é¡»(action-conditioned) |
| **å­¦ä¹ èŒƒå¼** | ç›‘ç£å­¦ä¹  | å˜åˆ†æ¨æ–­ + RL |

### 6.2 æ¶æ„ç»†èŠ‚

#### Encoder

| ç‰¹æ€§ | Traffic WM | DreamerV3 |
|------|-----------|-----------|
| **è¾“å…¥å¤„ç†** | å‘é‡ç‰¹å¾ | å›¾åƒ(CNN) + å‘é‡(MLP) |
| **å¤šæ™ºèƒ½ä½“** | âœ… Transformer attention | âŒ å•agentæˆ–å·²èšåˆ |
| **ç¦»æ•£ç‰¹å¾** | âœ… Embeddingå±‚(site, lane) | âš ï¸ éœ€é¢„å¤„ç†æˆ–one-hot |
| **è¾“å‡º** | ç¡®å®šæ€§latent | ç¡®å®šæ€§embed |

#### Dynamics

| ç‰¹æ€§ | Traffic WM | DreamerV3 RSSM |
|------|-----------|----------------|
| **çŠ¶æ€è¡¨ç¤º** | latent_dimç»´å‘é‡ | deter(ç¡®å®š) + stoch(éšæœº) |
| **æ—¶é—´æ¼”åŒ–** | RNN(GRU/LSTM/Transformer) | GRU + éšæœºé‡‡æ · |
| **éšæœºæ€§** | âŒ æ—  | âœ… Categorical/Gaussian |
| **Actionä¾èµ–** | âŒ æ—  | âœ… æœ‰ |
| **è®­ç»ƒæ–¹å¼** | Teacher forcing | Posterior-Prior KL |

#### Decoder

| ç‰¹æ€§ | Traffic WM | DreamerV3 |
|------|-----------|-----------|
| **è¾“å‡ºç±»å‹** | å‘é‡states + existence | å›¾åƒ + å‘é‡ + reward + cont |
| **é‡å»ºè´¨é‡** | MLP | ResNet (for images) |
| **è¾“å‡ºå½¢å¼** | ç¡®å®šæ€§(mean) | æ¦‚ç‡åˆ†å¸ƒ |
| **å¤šæ™ºèƒ½ä½“** | âœ… [K, F]è¾“å‡º | âŒ å•agent |

### 6.3 Losså‡½æ•°

| Lossç»„ä»¶ | Traffic WM | DreamerV3 |
|----------|-----------|-----------|
| **é‡å»ºloss** | Huber Loss | -log p(obs\|latent) |
| **é¢„æµ‹loss** | Huber Loss (1-step) | âŒ (é€šè¿‡KLéšå¼) |
| **KL loss** | âŒ æ—  | âœ… Dyn + Rep |
| **Existence** | âœ… BCE | âŒ (ç”¨cont head) |
| **Reward** | âŒ æ—  | âœ… (RLéœ€è¦) |

### 6.4 å®ç°æ¡†æ¶

| æ–¹é¢ | Traffic WM | DreamerV3 |
|------|-----------|-----------|
| **æ¡†æ¶** | PyTorch | JAX |
| **è‡ªåŠ¨å¾®åˆ†** | torch.autograd | jax.grad |
| **ç¼–è¯‘** | TorchScript (å¯é€‰) | JIT (é»˜è®¤) |
| **åˆ†å¸ƒå¼** | DDP | pmap |
| **éšæœºæ•°** | torch.random | nj.rng() |

---

## 7. ä¼˜ç¼ºç‚¹åˆ†æ

### 7.1 Traffic WM

#### ä¼˜ç‚¹ âœ…

1. **å¤šæ™ºèƒ½ä½“ç‰¹åŒ–**:
   - Transformer attentionè·¨agentèšåˆ
   - Masked poolingå¤„ç†å¯å˜æ•°é‡agent
   - Existence predictionæ˜¾å¼å»ºæ¨¡agentå‡ºç°/æ¶ˆå¤±

2. **ç®€å•é«˜æ•ˆ**:
   - çº¯ç¡®å®šæ€§æ¨¡å‹,è®­ç»ƒå¿«é€Ÿ
   - ç›´æ¥çš„encoder-dynamics-decoderç»“æ„
   - æ— éœ€å¤æ‚çš„KL balancing

3. **ç¦»æ•£ç‰¹å¾å¤„ç†**:
   - Embeddingå±‚å¤„ç†site_idå’Œlane_id
   - é¿å…é«˜ç»´one-hotç¼–ç 

4. **çµæ´»çš„dynamics**:
   - æ”¯æŒGRU/LSTM/Transformeré€‰æ‹©
   - é€‚åº”ä¸åŒæ—¶åºå¤æ‚åº¦

5. **è½¨è¿¹é¢„æµ‹ä¸“ç”¨**:
   - è®¾è®¡å¥‘åˆè½¦è¾†è½¨è¿¹é¢„æµ‹ä»»åŠ¡
   - è¾“å‡ºæ ¼å¼ç¬¦åˆè¯„ä¼°éœ€æ±‚(ADE/FDE)

#### ç¼ºç‚¹ âš ï¸

1. **æ— éšæœºæ€§**:
   - å®Œå…¨ç¡®å®šæ€§,æ— æ³•å»ºæ¨¡å¤šæ¨¡æ€æœªæ¥
   - å¯èƒ½æ¬ æ‹Ÿåˆå¤æ‚äº¤äº’åœºæ™¯
   - éš¾ä»¥æ•æ‰é©¾é©¶è¡Œä¸ºçš„ä¸ç¡®å®šæ€§

2. **Exposure Bias**:
   - è®­ç»ƒæ—¶ç”¨çœŸå®latentåºåˆ—
   - æµ‹è¯•æ—¶ç”¨é¢„æµ‹latent(ç´¯ç§¯è¯¯å·®)
   - å¯èƒ½å¯¼è‡´é•¿æœŸé¢„æµ‹æ¼‚ç§»

3. **ç¼ºå°‘æ­£åˆ™åŒ–**:
   - æ— latentç©ºé—´ç»“æ„çº¦æŸ
   - å¯èƒ½å­¦åˆ°ä¸è§„åˆ™çš„è¡¨ç¤º
   - æ³›åŒ–èƒ½åŠ›å¯èƒ½å—é™

4. **ä»…å‘é‡è¾“å…¥**:
   - æ— æ³•å¤„ç†å›¾åƒè¾“å…¥
   - ä¸èƒ½åˆ©ç”¨è§†è§‰ä¿¡æ¯(å¦‚é“è·¯å›¾)

5. **æ— actionå»ºæ¨¡**:
   - ä¸é€‚ç”¨äºå¼ºåŒ–å­¦ä¹ 
   - æ— æ³•åšwhat-ifåˆ†æ

### 7.2 DreamerV3

#### ä¼˜ç‚¹ âœ…

1. **å¼ºå¤§çš„è¡¨ç¤ºå­¦ä¹ **:
   - RSSMç»“åˆç¡®å®šæ€§å’Œéšæœºæ€§
   - Posterior-Prioråˆ†ç¦»å»ºæ¨¡è§‚æµ‹å’ŒåŠ¨æ€
   - KLæ­£åˆ™åŒ–å­¦ä¹ ç»“æ„åŒ–latentç©ºé—´

2. **å¤šæ¨¡æ€æ”¯æŒ**:
   - CNNå¤„ç†å›¾åƒ,MLPå¤„ç†å‘é‡
   - ç»Ÿä¸€æ¡†æ¶å¤„ç†ä¸åŒæ¨¡æ€

3. **æ¦‚ç‡å»ºæ¨¡**:
   - è¾“å‡ºåˆ†å¸ƒè€Œéå•ç‚¹ä¼°è®¡
   - å¯ä»¥é‡‡æ ·å¤šä¸ªæœªæ¥è½¨è¿¹
   - å»ºæ¨¡ä¸ç¡®å®šæ€§

4. **RL ready**:
   - Action-conditioned dynamics
   - Rewardå’Œcontinuationé¢„æµ‹
   - æ”¯æŒmodel-based planning

5. **ç†è®ºåŸºç¡€**:
   - åŸºäºå˜åˆ†æ¨æ–­
   - æœ‰ç†è®ºä¿è¯çš„å­¦ä¹ ç›®æ ‡

#### ç¼ºç‚¹ âš ï¸

1. **å¤æ‚æ€§é«˜**:
   - Priorå’ŒPosterioråŒåˆ†æ”¯
   - å¤šä¸ªlosséœ€è¦å¹³è¡¡æƒé‡
   - è®­ç»ƒä¸ç¨³å®šé£é™©

2. **è®¡ç®—å¼€é”€**:
   - ResNetç¼–è§£ç å™¨é‡
   - é‡‡æ ·æ“ä½œå¢åŠ è®¡ç®—
   - éœ€è¦æ›´å¤šå†…å­˜

3. **è¶…å‚æ•°æ•æ„Ÿ**:
   - KL lossæƒé‡
   - Free bitsé˜ˆå€¼
   - å¤šä¸ªloss scaleéœ€è¦è°ƒä¼˜

4. **æ— å¤šæ™ºèƒ½ä½“**:
   - å‡è®¾å•agentè§‚æµ‹
   - éœ€è¦é¢å¤–è®¾è®¡å¤„ç†å¤šagent
   - éš¾ä»¥ç›´æ¥åº”ç”¨äºäº¤é€šåœºæ™¯

5. **Actionä¾èµ–**:
   - å¿…é¡»æœ‰actionè¾“å…¥
   - ä¸é€‚ç”¨äºçº¯è§‚æµ‹é¢„æµ‹
   - å¢åŠ æ•°æ®éœ€æ±‚

---

## 8. æ”¹è¿›å»ºè®®

### 8.1 ä¸ºTraffic WMæ·»åŠ éšæœºæ€§

**æ–¹æ¡ˆ1: ç®€åŒ–ç‰ˆRSSM**

```python
class StochasticLatentDynamics(nn.Module):
    def __init__(self, latent_dim, stoch_dim, hidden_dim):
        # ç¡®å®šæ€§éƒ¨åˆ†
        self.gru = nn.GRU(latent_dim, hidden_dim)

        # éšæœºéƒ¨åˆ† (prior)
        self.prior_net = nn.Linear(hidden_dim, stoch_dim * 2)  # mean, logvar

        # éšæœºéƒ¨åˆ† (posterior) - ç”¨äºè®­ç»ƒ
        self.posterior_net = nn.Linear(hidden_dim + latent_dim, stoch_dim * 2)

    def forward(self, latent, hidden=None, use_posterior=False, next_latent=None):
        # 1. æ›´æ–°ç¡®å®šæ€§çŠ¶æ€
        deter, hidden = self.gru(latent, hidden)

        # 2. é¢„æµ‹éšæœºçŠ¶æ€
        if use_posterior and next_latent is not None:
            # Posterior: q(z_t | h_t, x_t)
            cat = torch.cat([deter, next_latent], -1)
            post_params = self.posterior_net(cat)
            mean, logvar = post_params.chunk(2, -1)
            stoch = self.reparameterize(mean, logvar)

            # Prior: p(z_t | h_t)
            prior_params = self.prior_net(deter)
            prior_mean, prior_logvar = prior_params.chunk(2, -1)

            kl_loss = self.kl_divergence(
                (mean, logvar), (prior_mean, prior_logvar)
            )

            return torch.cat([deter, stoch], -1), hidden, kl_loss
        else:
            # Prior only (inference)
            prior_params = self.prior_net(deter)
            mean, logvar = prior_params.chunk(2, -1)
            stoch = self.reparameterize(mean, logvar)

            return torch.cat([deter, stoch], -1), hidden, None
```

**ä¼˜ç‚¹**:
- ä¿æŒå½“å‰æ¶æ„åŸºæœ¬ä¸å˜
- æ·»åŠ å¤šæ¨¡æ€é¢„æµ‹èƒ½åŠ›
- ç†è®ºä¸Šæ›´robust

**ç¼ºç‚¹**:
- å¢åŠ è®­ç»ƒå¤æ‚åº¦
- éœ€è¦è°ƒæ•´KL lossæƒé‡

### 8.2 ä¸ºDreamerV3æ·»åŠ å¤šæ™ºèƒ½ä½“æ”¯æŒ

**æ–¹æ¡ˆ: Multi-Agent RSSM**

```python
class MultiAgentRSSM(nj.Module):
    def __init__(self, deter, stoch, classes, max_agents):
        self.max_agents = max_agents
        self.rssm = RSSM(deter, stoch, classes)

        # Agent aggregation
        self.agent_attention = TransformerEncoder(...)

    def obs_step(self, prev_state, prev_action, embed, masks):
        """
        embed: [B, K, embed_dim] - per-agent embeddings
        masks: [B, K] - agentå­˜åœ¨mask
        """

        # 1. Per-agent RSSM
        agent_posts = []
        agent_priors = []

        for k in range(self.max_agents):
            if masks[:, k].any():
                post, prior = self.rssm.obs_step(
                    prev_state[k], prev_action, embed[:, k], ...
                )
                agent_posts.append(post)
                agent_priors.append(prior)

        # 2. Aggregate via attention
        aggregated = self.agent_attention(agent_posts, masks)

        return aggregated, agent_posts, agent_priors
```

---

## 9. æ€»ç»“

### æ ¸å¿ƒå·®å¼‚çŸ©é˜µ

| ç»´åº¦ | Traffic WM | DreamerV3 | æœ€ä½³åº”ç”¨ |
|------|-----------|-----------|---------|
| **ä»»åŠ¡ç±»å‹** | è½¨è¿¹é¢„æµ‹ | Model-based RL | Traffic: é¢„æµ‹<br>Dreamer: å†³ç­– |
| **ä¸ç¡®å®šæ€§** | ç¡®å®šæ€§ | éšæœºæ€§(RSSM) | Dreameræ›´robust |
| **å¤šæ™ºèƒ½ä½“** | âœ… åŸç”Ÿæ”¯æŒ | âŒ éœ€æ‰©å±• | Trafficèƒœå‡º |
| **è®¡ç®—æ•ˆç‡** | é«˜(ç®€å•MLP) | ä¸­(ResNet+é‡‡æ ·) | Trafficæ›´å¿« |
| **ç†è®ºåŸºç¡€** | ç›‘ç£å­¦ä¹  | å˜åˆ†æ¨æ–­ | Dreameræ›´ä¸¥è°¨ |
| **å¯è§£é‡Šæ€§** | é«˜(ç¡®å®šæ˜ å°„) | ä¸­(æ¦‚ç‡æ¨¡å‹) | Trafficæ›´ç›´è§‚ |

### æœ€ç»ˆå»ºè®®

**ç»§ç»­ä½¿ç”¨Traffic WMçš„åœºæ™¯**:
- âœ… çº¯è½¨è¿¹é¢„æµ‹ä»»åŠ¡
- âœ… éœ€è¦å¿«é€Ÿè®­ç»ƒå’Œæ¨ç†
- âœ… å¤šæ™ºèƒ½ä½“äº¤äº’å»ºæ¨¡
- âœ… è®¡ç®—èµ„æºæœ‰é™

**è€ƒè™‘å€Ÿé‰´DreamerV3çš„åœºæ™¯**:
- ğŸ”„ éœ€è¦å»ºæ¨¡å¤šæ¨¡æ€æœªæ¥
- ğŸ”„ éœ€è¦ä¸ç¡®å®šæ€§é‡åŒ–
- ğŸ”„ æœ‰å›¾åƒè¾“å…¥éœ€æ±‚
- ğŸ”„ è®¡åˆ’åšå¼ºåŒ–å­¦ä¹ æ‰©å±•

**æ··åˆæ–¹æ¡ˆ**:
- ä¿æŒTraffic WMçš„å¤šæ™ºèƒ½ä½“encoder
- æ·»åŠ ç®€åŒ–ç‰ˆRSSM dynamics (ä»…posterior-prior,æ— discrete)
- ä¿æŒç®€å•çš„MLP decoder
- æ·»åŠ KL regularization

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0
**ç”Ÿæˆæ—¥æœŸ**: 2025-12-14
**ä½œè€…**: Claude Code Analysis
**é¡¹ç›®**: Traffic World Model vs DreamerV3
